{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEKzpp7Nl7Pm",
        "outputId": "e45a3be7-090f-4966-c306-7da429c549db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "VhL0imesl9gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_drive_path = '/content/drive/MyDrive'\n",
        "CHUNK_SIZE = 18\n",
        "\n",
        "wiki_dir = os.path.join(base_drive_path, 'wiki-pages')\n",
        "train_path = os.path.join(base_drive_path, 'train.jsonl')\n",
        "dev_path = os.path.join(base_drive_path, 'shared_task_dev.jsonl')\n",
        "dev_public_path = os.path.join(base_drive_path, 'shared_task_dev_public.jsonl')\n",
        "\n",
        "output_dir = os.path.join(base_drive_path, 'newones/processed_data')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# ===== HELPERS =====\n",
        "def log(message):\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\")"
      ],
      "metadata": {
        "id": "3fLZNYNbl9iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== LOAD WIKI CHUNK =====\n",
        "def load_wiki_dumps_chunk(wiki_files_chunk):\n",
        "    wiki_dict = {}\n",
        "    total_loaded_pages = 0\n",
        "\n",
        "    for wiki_file in tqdm(wiki_files_chunk, desc=\"Loading wiki chunk\"):\n",
        "        if not os.path.exists(wiki_file):\n",
        "            log(f\"Warning: File not found at {wiki_file}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(wiki_file, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        page = json.loads(line)\n",
        "                        page_id = str(page.get('id', ''))\n",
        "                        lines_str = page.get('lines', '')\n",
        "                        sentences = []\n",
        "                        if lines_str:\n",
        "                            for ln in lines_str.strip().split('\\n'):\n",
        "                                parts = ln.split('\\t', 1)\n",
        "                                if len(parts) == 2:\n",
        "                                    sentences.append(parts[1])\n",
        "                            wiki_dict[page_id] = sentences\n",
        "                            total_loaded_pages += 1\n",
        "                    except json.JSONDecodeError:\n",
        "                        log(f\"Error decoding JSON in {wiki_file}, skipping line.\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error reading file {wiki_file}: {e}\")\n",
        "\n",
        "    log(f\"Loaded {total_loaded_pages} wiki pages from this chunk.\")\n",
        "    return wiki_dict"
      ],
      "metadata": {
        "id": "anFTnOp9pszg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== PROCESS DATASET =====\n",
        "def process_and_append_dataset(dataset, remaining_indices, wiki_dict, output_path):\n",
        "    newly_completed = set()\n",
        "\n",
        "    for idx in list(remaining_indices):\n",
        "        example = dataset[idx]\n",
        "\n",
        "        if example.get(\"label\") == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        evidences_text = []\n",
        "        evidences = example.get('evidence', [])\n",
        "        found_any = False\n",
        "\n",
        "        for evidence_set in evidences:\n",
        "            evidence_sentences = []\n",
        "            for ev in evidence_set:\n",
        "                page_title = ev[2]\n",
        "                sent_id = ev[3]\n",
        "\n",
        "                if page_title and sent_id is not None:\n",
        "                    page_title_str = str(page_title)\n",
        "                    if page_title_str in wiki_dict:\n",
        "                        sentences = wiki_dict[page_title_str]\n",
        "                        if isinstance(sent_id, int) and 0 <= sent_id < len(sentences):\n",
        "                            evidence_sentences.append(sentences[sent_id])\n",
        "                            found_any = True\n",
        "            evidences_text.append(evidence_sentences)\n",
        "\n",
        "        # If we found any evidence text, mark as completed\n",
        "        if found_any:\n",
        "            example['evidence_text'] = evidences_text\n",
        "            newly_completed.add(idx)\n",
        "\n",
        "    # Write only newly completed examples\n",
        "    if newly_completed:\n",
        "        with open(output_path, 'a', encoding='utf-8') as f_out:\n",
        "            for idx in newly_completed:\n",
        "                f_out.write(json.dumps(dataset[idx]) + \"\\n\")\n",
        "            f_out.flush()\n",
        "            os.fsync(f_out.fileno())  # Force write to Drive\n",
        "\n",
        "    return newly_completed\n"
      ],
      "metadata": {
        "id": "m0AYccEQps1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== MAIN PROCESS (Handles both train & dev) =====\n",
        "def main(dataset_path, output_dir, wiki_dir, chunk_size=CHUNK_SIZE):\n",
        "    output_path = os.path.join(output_dir, os.path.basename(dataset_path))\n",
        "\n",
        "    # Clear old output if exists\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "        log(f\"Cleared previous output file: {output_path}\")\n",
        "\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    # Pre-handle \"NOT ENOUGH INFO\" cases\n",
        "    remaining_indices = set()\n",
        "    with open(output_path, 'a', encoding='utf-8') as f_out:\n",
        "        for idx, example in enumerate(dataset):\n",
        "            if example.get(\"label\") == \"NOT ENOUGH INFO\":\n",
        "                example[\"evidence_text\"] = [[]]\n",
        "                f_out.write(json.dumps(example) + \"\\n\")\n",
        "            else:\n",
        "                remaining_indices.add(idx)\n",
        "        f_out.flush()\n",
        "        os.fsync(f_out.fileno())\n",
        "\n",
        "    # Prepare wiki files list\n",
        "    all_wiki_files = sorted(glob.glob(os.path.join(wiki_dir, 'wiki-*.jsonl')))\n",
        "    num_chunks = math.ceil(len(all_wiki_files) / chunk_size)\n",
        "\n",
        "    log(f\"Total wiki files found: {len(all_wiki_files)}\")\n",
        "    log(f\"Processing {os.path.basename(dataset_path)} in {num_chunks} chunks (size {chunk_size})...\")\n",
        "\n",
        "    # Iterate over wiki chunks\n",
        "    for i in range(num_chunks):\n",
        "        if not remaining_indices:\n",
        "            log(\"✅ All examples processed early, stopping.\")\n",
        "            break\n",
        "\n",
        "        start_index = i * chunk_size\n",
        "        end_index = start_index + chunk_size\n",
        "        current_wiki_chunk_files = all_wiki_files[start_index:end_index]\n",
        "\n",
        "        log(f\"--- Chunk {i+1}/{num_chunks} (files {start_index+1} to {end_index}) ---\")\n",
        "\n",
        "        # Load current chunk\n",
        "        wiki_data_chunk = load_wiki_dumps_chunk(current_wiki_chunk_files)\n",
        "\n",
        "        # Process only unprocessed examples\n",
        "        newly_completed = process_and_append_dataset(dataset, remaining_indices, wiki_data_chunk, output_path)\n",
        "\n",
        "        # Remove completed examples from the set\n",
        "        remaining_indices -= newly_completed\n",
        "\n",
        "        log(f\"Completed {len(newly_completed)} new examples this chunk. Remaining: {len(remaining_indices)}\")\n",
        "\n",
        "        # Free memory\n",
        "        del wiki_data_chunk\n",
        "        gc.collect()\n",
        "\n",
        "    log(f\"✅ Processing complete! Output saved to: {output_path}\")\n",
        "\n",
        "\n",
        "# ===== RUN =====\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage for train\n",
        "    main(train_path, output_dir, wiki_dir)\n",
        "    # Example usage for dev\n",
        "    main(dev_path, output_dir, wiki_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FstPVUfwps3i",
        "outputId": "f4c8bcfc-b907-448e-b7be-cc9218e39856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:10:59] Total wiki files found: 109\n",
            "[2025-08-10 14:10:59] Processing train.jsonl in 7 chunks (size 18)...\n",
            "[2025-08-10 14:10:59] --- Chunk 1/7 (files 1 to 18) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:26<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:11:26] Loaded 892924 wiki pages from this chunk.\n",
            "[2025-08-10 14:11:27] Completed 19397 new examples this chunk. Remaining: 90413\n",
            "[2025-08-10 14:11:29] --- Chunk 2/7 (files 19 to 36) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:24<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:11:53] Loaded 898349 wiki pages from this chunk.\n",
            "[2025-08-10 14:11:54] Completed 18828 new examples this chunk. Remaining: 71585\n",
            "[2025-08-10 14:11:56] --- Chunk 3/7 (files 37 to 54) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:21<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:12:17] Loaded 898511 wiki pages from this chunk.\n",
            "[2025-08-10 14:12:18] Completed 20372 new examples this chunk. Remaining: 51213\n",
            "[2025-08-10 14:12:20] --- Chunk 4/7 (files 55 to 72) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:23<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:12:43] Loaded 892736 wiki pages from this chunk.\n",
            "[2025-08-10 14:12:44] Completed 16175 new examples this chunk. Remaining: 35038\n",
            "[2025-08-10 14:12:46] --- Chunk 5/7 (files 73 to 90) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:22<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:13:09] Loaded 898553 wiki pages from this chunk.\n",
            "[2025-08-10 14:13:09] Completed 15120 new examples this chunk. Remaining: 19918\n",
            "[2025-08-10 14:13:11] --- Chunk 6/7 (files 91 to 108) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:22<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:13:33] Loaded 898510 wiki pages from this chunk.\n",
            "[2025-08-10 14:13:34] Completed 19179 new examples this chunk. Remaining: 739\n",
            "[2025-08-10 14:13:36] --- Chunk 7/7 (files 109 to 126) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:13:36] Loaded 16523 wiki pages from this chunk.\n",
            "[2025-08-10 14:13:36] Completed 64 new examples this chunk. Remaining: 675\n",
            "[2025-08-10 14:13:38] ✅ Processing complete! Output saved to: /content/drive/MyDrive/newones/processed_data/train.jsonl\n",
            "[2025-08-10 14:13:38] Total wiki files found: 109\n",
            "[2025-08-10 14:13:38] Processing shared_task_dev.jsonl in 7 chunks (size 18)...\n",
            "[2025-08-10 14:13:38] --- Chunk 1/7 (files 1 to 18) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:23<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:14:02] Loaded 892924 wiki pages from this chunk.\n",
            "[2025-08-10 14:14:02] Completed 2319 new examples this chunk. Remaining: 11013\n",
            "[2025-08-10 14:14:04] --- Chunk 2/7 (files 19 to 36) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:20<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:14:24] Loaded 898349 wiki pages from this chunk.\n",
            "[2025-08-10 14:14:24] Completed 2328 new examples this chunk. Remaining: 8685\n",
            "[2025-08-10 14:14:25] --- Chunk 3/7 (files 37 to 54) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:22<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:14:47] Loaded 898511 wiki pages from this chunk.\n",
            "[2025-08-10 14:14:47] Completed 2204 new examples this chunk. Remaining: 6481\n",
            "[2025-08-10 14:14:49] --- Chunk 4/7 (files 55 to 72) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:20<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:15:09] Loaded 892736 wiki pages from this chunk.\n",
            "[2025-08-10 14:15:10] Completed 1741 new examples this chunk. Remaining: 4740\n",
            "[2025-08-10 14:15:11] --- Chunk 5/7 (files 73 to 90) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:21<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:15:32] Loaded 898553 wiki pages from this chunk.\n",
            "[2025-08-10 14:15:32] Completed 2038 new examples this chunk. Remaining: 2702\n",
            "[2025-08-10 14:15:34] --- Chunk 6/7 (files 91 to 108) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 18/18 [00:21<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:15:55] Loaded 898510 wiki pages from this chunk.\n",
            "[2025-08-10 14:15:55] Completed 2593 new examples this chunk. Remaining: 109\n",
            "[2025-08-10 14:15:57] --- Chunk 7/7 (files 109 to 126) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading wiki chunk: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-10 14:15:57] Loaded 16523 wiki pages from this chunk.\n",
            "[2025-08-10 14:15:57] Completed 6 new examples this chunk. Remaining: 103\n",
            "[2025-08-10 14:15:58] ✅ Processing complete! Output saved to: /content/drive/MyDrive/newones/processed_data/shared_task_dev.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uH-Qp30Xl9nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUG1gOOSl9pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4voKt2ohl9ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fj1oWFIOl9u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DMFXMw96l91x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ArON-SEl95W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}