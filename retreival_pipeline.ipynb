{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR_b-EMxl0Tf",
        "outputId": "e1e972c3-84bc-4654-80c1-3fbc44b913a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Cell 1 â€“ Install and Setup\n",
        "# ===============================\n",
        "\n",
        "!pip install rank-bm25 sentence-transformers spacy faiss-cpu fuzzywuzzy python-Levenshtein --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import spacy\n",
        "import faiss\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load spacy for NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load sentence embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 2 â€“ Mount Drive & Build Page Index\n",
        "# ===============================\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "WIKI_PATH = \"/content/drive/MyDrive/wiki-pages\"\n",
        "\n",
        "page_index = {}   # page_id -> file path\n",
        "title_index = {}  # preprocessed page_id -> page_id\n",
        "\n",
        "print(\"Indexing pages...\")\n",
        "for fname in tqdm(os.listdir(WIKI_PATH)):\n",
        "    if not fname.endswith(\".json\") and not fname.endswith(\".jsonl\"):\n",
        "        continue\n",
        "    fpath = os.path.join(WIKI_PATH, fname)\n",
        "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                pid = data[\"id\"]\n",
        "                page_index[pid] = fpath\n",
        "                pre_title = pid.replace(\"_\", \" \").lower()\n",
        "                title_index[pre_title] = pid\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "print(f\"âœ… Indexed {len(page_index)} pages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3E5z9phl_yJ",
        "outputId": "29257189-916d-40b3-bdfb-d517fc3b86e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Indexing pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [02:20<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Indexed 5416537 pages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building FAISS IVF index...\")\n",
        "\n",
        "titles_list = list(title_index.keys())\n",
        "ids_list = [title_index[t] for t in titles_list]\n",
        "\n",
        "dimension = 384\n",
        "nlist = 1000\n",
        "quantizer = faiss.IndexFlatL2(dimension)\n",
        "ivf_index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
        "\n",
        "train_limit = min(5000, len(titles_list))\n",
        "train_embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, train_limit, 256), desc=\"Training IVF\", unit=\"batch\"):\n",
        "    batch = titles_list[i:i+256]\n",
        "    emb = embedder.encode(batch, convert_to_tensor=True).cpu().numpy().astype(\"float32\")\n",
        "    train_embeddings.append(emb)\n",
        "\n",
        "train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
        "ivf_index.train(train_embeddings)\n",
        "del train_embeddings\n",
        "\n",
        "for i in tqdm(range(0, len(titles_list), 256), desc=\"Adding embeddings\", unit=\"batch\"):\n",
        "    batch = titles_list[i:i+256]\n",
        "    emb = embedder.encode(batch, convert_to_tensor=True).cpu().numpy().astype(\"float32\")\n",
        "    ivf_index.add(emb)\n",
        "\n",
        "try:\n",
        "    res = faiss.StandardGpuResources()\n",
        "    faiss_index = faiss.index_cpu_to_gpu(res, 0, ivf_index)\n",
        "    print(\"âœ… FAISS index on GPU.\")\n",
        "except:\n",
        "    faiss_index = ivf_index\n",
        "    print(\"âœ… FAISS index on CPU.\")\n",
        "\n",
        "faiss.write_index(faiss_index, \"faiss_index_ivf.bin\")\n",
        "print(f\"FAISS IVF index built with {len(titles_list)} titles.\")\n"
      ],
      "metadata": {
        "id": "3mH1OADml_02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6178574c-22aa-4abe-d848-c9271edd702b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building FAISS IVF index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training IVF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.62batch/s]\n",
            "Adding embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21136/21136 [22:17<00:00, 15.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… FAISS index on CPU.\n",
            "FAISS IVF index built with 5410691 titles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 4 â€“ Helpers\n",
        "# ===============================\n",
        "\n",
        "def get_page(page_id):\n",
        "    fpath = page_index.get(page_id, None)\n",
        "    if not fpath:\n",
        "        return None\n",
        "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            if data[\"id\"] == page_id:\n",
        "                return data\n",
        "    return None\n",
        "\n",
        "def extract_sentences(page):\n",
        "    for line in page[\"lines\"].split(\"\\n\"):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        parts = line.split(\"\\t\")\n",
        "        if len(parts) >= 2:\n",
        "            sent_id, sent_text = parts[0], parts[1]\n",
        "            yield (sent_text, sent_id)  # generator yields one by one to save RAM\n"
      ],
      "metadata": {
        "id": "IjgmKjyrl_36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 5 â€“ Candidate Retrieval via FAISS\n",
        "# ===============================\n",
        "\n",
        "def extract_candidates_with_faiss(query, top_k=5):\n",
        "    query_emb = embedder.encode(query, convert_to_tensor=True).cpu().numpy().astype('float32').reshape(1, -1)\n",
        "    distances, indices = faiss_index.search(query_emb, top_k)\n",
        "    candidates = [ids_list[idx] for idx in indices[0] if idx < len(ids_list)]\n",
        "    return candidates\n"
      ],
      "metadata": {
        "id": "NkIZSI5Jl_6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 6 â€“ Hybrid Retrieval (BM25 + Embeddings)\n",
        "# ===============================\n",
        "\n",
        "# def hybrid_retrieval(query, sentences_gen, top_k=3, alpha=0.6):\n",
        "#     corpus, sent_ids = [], []\n",
        "#     for sent, sid in sentences_gen:\n",
        "#         corpus.append(sent)\n",
        "#         sent_ids.append(sid)\n",
        "\n",
        "#     if not corpus:\n",
        "#         return []\n",
        "\n",
        "#     tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "#     bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "#     bm25_scores = bm25.get_scores(query.split(\" \"))\n",
        "#     query_emb = embedder.encode(query, convert_to_tensor=True)\n",
        "#     doc_embs = embedder.encode(corpus, convert_to_tensor=True)\n",
        "#     cosine_scores = util.cos_sim(query_emb, doc_embs)[0].cpu().numpy()\n",
        "\n",
        "#     # Normalize\n",
        "#     bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-8)\n",
        "#     cos_norm = (cosine_scores - cosine_scores.min()) / (cosine_scores.max() - cosine_scores.min() + 1e-8)\n",
        "#     final_scores = alpha * cos_norm + (1 - alpha) * bm25_norm\n",
        "\n",
        "#     ranked = sorted(zip(corpus, final_scores), key=lambda x: x[1], reverse=True)\n",
        "#     return ranked[:top_k]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9X0zebmQl_92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 7 â€“ Full Retrieval Pipeline\n",
        "# ===============================\n",
        "\n",
        "def retrieve_evidence_auto(query, top_k=3, alpha=0.6):\n",
        "    candidate_titles = extract_candidates_with_faiss(query, top_k=5)\n",
        "    print(f\"Candidate titles for query: {candidate_titles}\")\n",
        "\n",
        "    # Lazy sentence extraction\n",
        "    all_sentences_gen = (s for title in candidate_titles\n",
        "                         for s in extract_sentences(get_page(title)))\n",
        "\n",
        "    return hybrid_retrieval(query, all_sentences_gen, top_k=top_k, alpha=alpha)"
      ],
      "metadata": {
        "id": "KBg6VYyHmAAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ===============================\n",
        "# Example Queries (FEVER style)\n",
        "# ===============================\n",
        "queries = [\n",
        "    \"Barack Obama was President of the United States in 2010.\",\n",
        "    \"The Berlin Wall fell in 1989.\",\n",
        "    \"Titanic was directed by James Cameron.\",\n",
        "    \"Mount Everest is the highest mountain on Earth.\",\n",
        "    \"Roger Federer has won 20 Grand Slam singles titles.\"\n",
        "]\n",
        "# ===============================\n",
        "# Retrieval Loop - Improved UI\n",
        "# ===============================\n",
        "for idx, q in enumerate(queries, 1):\n",
        "    print(f\"\\n===============================\")\n",
        "    print(f\"ðŸ”Ž Query {idx}: {q}\")\n",
        "    print(f\"===============================\")\n",
        "\n",
        "    # Retrieve top-3 candidate evidence sentences\n",
        "    results = retrieve_evidence_auto(q, top_k=5, alpha=0.6)\n",
        "\n",
        "    for rank, (sent, score) in enumerate(results, 1):\n",
        "        # Extract possible wiki page refs like [Page_Name]\n",
        "        pages = re.findall(r\"\\[([^\\]]+)\\]\", sent)\n",
        "\n",
        "        # Clean up artifacts in retrieved text\n",
        "        clean_sent = re.sub(r\"\\s*--\\s*\", \"â€“\", sent)   # replace -- with â€“\n",
        "        clean_sent = clean_sent.replace(\"-LRB-\", \"(\").replace(\"-RRB-\", \")\")\n",
        "\n",
        "        # Structured display\n",
        "        print(f\"  {rank}. Evidence {'ðŸ“„' if pages else ''}:\")\n",
        "        print(f\"     âž¡ï¸ Text : {clean_sent}\")\n",
        "        if pages:\n",
        "            print(f\"     ðŸ·ï¸ Source: {', '.join(pages)}\")\n",
        "        print(f\"     ðŸ”¹ Score : {score:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "Z-cvPhdKmADX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81935bbe-5cdb-4be7-8953-553a4c6cd7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================\n",
            "ðŸ”Ž Query 1: Barack Obama was President of the United States in 2010.\n",
            "===============================\n",
            "Candidate titles for query: ['Barack_Obama_Sr.', 'Presidential_transition_of_Barack_Obama', 'Barack_Obama_presidential_campaign', 'Baracksdubs', 'Baracktrema_obamai']\n",
            "  1. Evidence :\n",
            "     âž¡ï¸ Text : Barack Obama , the 44th President of the United States , has successfully run for president twice :\n",
            "     ðŸ”¹ Score : 0.9925\n",
            "\n",
            "  2. Evidence :\n",
            "     âž¡ï¸ Text : The Presidential transition of Barack Obama began when Barack Obama won the United States presidential election on November 4 , 2008 , and became the President-elect .\n",
            "     ðŸ”¹ Score : 0.8778\n",
            "\n",
            "  3. Evidence :\n",
            "     âž¡ï¸ Text : Barack Hussein Obama Sr. ( -LSB- ËˆbÃ¦rÉ™k_huËËˆseÉªn_oÊŠËˆbÉ‘ËmÉ™ -RSB- 18 June 1936â€“24 November 1982 ) was a Kenyan senior governmental economist and the father of Barack Obama , the 44th President of the United States .\n",
            "     ðŸ”¹ Score : 0.8744\n",
            "\n",
            "  4. Evidence :\n",
            "     âž¡ï¸ Text : Barack Obama presidential campaign , 2008\n",
            "     ðŸ”¹ Score : 0.7224\n",
            "\n",
            "  5. Evidence :\n",
            "     âž¡ï¸ Text : Barack Obama presidential campaign , 2012\n",
            "     ðŸ”¹ Score : 0.7083\n",
            "\n",
            "\n",
            "===============================\n",
            "ðŸ”Ž Query 2: The Berlin Wall fell in 1989.\n",
            "===============================\n",
            "Candidate titles for query: ['Timeline_of_Berlin', 'Berlin_Secession', '1991_in_Germany', '1989_in_Romania', 'History_of_Germany_since_1990']\n",
            "  1. Evidence :\n",
            "     âž¡ï¸ Text : The following is a timeline of the history of the city of Berlin , Germany .\n",
            "     ðŸ”¹ Score : 0.8971\n",
            "\n",
            "  2. Evidence :\n",
            "     âž¡ï¸ Text : The Berlin Secession ( Berliner Secession ) was an art association founded by Berlin artists in 1898 as an alternative to the conservative state-run Association of Berlin Artists .\n",
            "     ðŸ”¹ Score : 0.7847\n",
            "\n",
            "  3. Evidence :\n",
            "     âž¡ï¸ Text : Events in the year 1991 in Germany .\n",
            "     ðŸ”¹ Score : 0.7826\n",
            "\n",
            "  4. Evidence :\n",
            "     âž¡ï¸ Text : The biggest conflict in the Berlin Secession was over the question of whether it should follow the new wave of Expressionism .\n",
            "     ðŸ”¹ Score : 0.7397\n",
            "\n",
            "  5. Evidence :\n",
            "     âž¡ï¸ Text : Germany after 1990 is referred to by historians as the Berlin Republic .\n",
            "     ðŸ”¹ Score : 0.7166\n",
            "\n",
            "\n",
            "===============================\n",
            "ðŸ”Ž Query 3: Titanic was directed by James Cameron.\n",
            "===============================\n",
            "Candidate titles for query: ['Fiction_Factory_Films', 'List_of_British_films_of_2008', 'Films_Albatros', 'Verity_Films', 'Cannes_Soundtrack_Award']\n",
            "  1. Evidence :\n",
            "     âž¡ï¸ Text : Verity Films was a British documentary film production company , founded by Sydney Box and Jay Gardner Lewis in March or May 1940 .\n",
            "     ðŸ”¹ Score : 0.8398\n",
            "\n",
            "  2. Evidence :\n",
            "     âž¡ï¸ Text : Films Albatros was a French film production company established in 1922 .\n",
            "     ðŸ”¹ Score : 0.7845\n",
            "\n",
            "  3. Evidence :\n",
            "     âž¡ï¸ Text : The Cannes Soundtrack Award is an independent award of the Cannes Film Festival bestowed by the jury of the festival on one of the competing feature films .\n",
            "     ðŸ”¹ Score : 0.6438\n",
            "\n",
            "  4. Evidence :\n",
            "     âž¡ï¸ Text : A list of British films released in 2008 .\n",
            "     ðŸ”¹ Score : 0.6000\n",
            "\n",
            "  5. Evidence :\n",
            "     âž¡ï¸ Text : Fiction Factory Films , also known as Fiction Factory , is a television production company based in Cardiff .\n",
            "     ðŸ”¹ Score : 0.4591\n",
            "\n",
            "\n",
            "===============================\n",
            "ðŸ”Ž Query 4: Mount Everest is the highest mountain on Earth.\n",
            "===============================\n",
            "Candidate titles for query: ['Mount_Everest', 'Mount_Everest_Nepal', 'Mount_Everest_in_2012', 'The_Everest', '2008_Summer_Olympics_summit_of_Mt._Everest']\n",
            "  1. Evidence :\n",
            "     âž¡ï¸ Text : Mount Everest in 2012 is about the events on and surrounding Mount Everest , the highest elevation mountain on Earth .\n",
            "     ðŸ”¹ Score : 0.9466\n",
            "\n",
            "  2. Evidence :\n",
            "     âž¡ï¸ Text : Mount Everest , also known in Nepal as SagarmÄthÄ and in China as Chomolungma , is Earth 's highest mountain .\n",
            "     ðŸ”¹ Score : 0.8663\n",
            "\n",
            "  3. Evidence :\n",
            "     âž¡ï¸ Text : Mount Everest is in the Mahalangur Range .\n",
            "     ðŸ”¹ Score : 0.7055\n",
            "\n",
            "  4. Evidence :\n",
            "     âž¡ï¸ Text : In 2010 , an agreement was finally reached by both sides that the height of Everest is 8,848 m , and Nepal recognises China 's claim that the rock height of Everest is 8,844 m.\n",
            "     ðŸ”¹ Score : 0.5603\n",
            "\n",
            "  5. Evidence :\n",
            "     âž¡ï¸ Text : Mount Everest Nepal was a Palauan association football club which competed in the Palau Soccer League , the top level league in Palau , for the first time in the inaugural season in 2004 , when they were runners up , losing 2-0 to Daewoo Ngatpang in the final .\n",
            "     ðŸ”¹ Score : 0.5268\n",
            "\n",
            "\n",
            "===============================\n",
            "ðŸ”Ž Query 5: Roger Federer has won 20 Grand Slam singles titles.\n",
            "===============================\n",
            "Candidate titles for query: [\"List_of_Grand_Slam_men's_singles_champions\", \"1986_Wimbledon_Championships_â€“_Men's_Singles\", \"1984_Wimbledon_Championships_â€“_Men's_Singles\", '2008_Hall_of_Fame_Tennis_Championships_â€“_Singles', \"1995_Wimbledon_Championships_â€“_Men's_Singles\"]\n",
            "  1. Evidence :\n",
            "     âž¡ï¸ Text : This article details the list of men 's singles Grand Slam tournaments tennis champions .\n",
            "     ðŸ”¹ Score : 1.0000\n",
            "\n",
            "  2. Evidence :\n",
            "     âž¡ï¸ Text : Future champion Boris Becker debuted on his first Grand Slam appearance and would win the title the following year .\n",
            "     ðŸ”¹ Score : 0.7256\n",
            "\n",
            "  3. Evidence :\n",
            "     âž¡ï¸ Text : Pete Sampras was the two-time defending champion and successfully defended his title , defeating Boris Becker 6â€“7 ( 5â€“7 ) , 6â€“2 , 6â€“4 , 6â€“2 in the final to win the gentlemen 's singles title at the 1995 Wimbledon Championships .\n",
            "     ðŸ”¹ Score : 0.5139\n",
            "\n",
            "  4. Evidence :\n",
            "     âž¡ï¸ Text : Boris Becker was the defending champion and defeated Ivan Lendl 6â€“4 , 6â€“3 , 7â€“5 in the final to win the Gentlemen 's Singles tennis title at the 1986 Wimbledon Championships , his second consecutive .\n",
            "     ðŸ”¹ Score : 0.4698\n",
            "\n",
            "  5. Evidence :\n",
            "     âž¡ï¸ Text : John McEnroe was the defending champion and won in the final 6â€“1 , 6â€“1 , 6â€“2 against Jimmy Connors .\n",
            "     ðŸ”¹ Score : 0.3693\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8Bd5FBQmAGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdW9Kh_rmAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erCJ3apXmALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JbEKhHamAON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkqBJv-amAQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2AKM553mAUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}